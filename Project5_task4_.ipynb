{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK6xl1pHdXQmoOrIhEe/kr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alicetw40342/PDF-Based-Question-Answering-System-with-RAG-and-Gemini---LLM-Temperature-and-Tokens/blob/main/Project5_task4_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ1nHpnYtKST"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“„ findings.txt - Summary of Temperature and Token Experiments\n",
        "\n",
        "1. How did the output change as you increased the temperature?\n",
        "As the temperature increased from 0.0 to 1.0, the responses became more diverse and creative. At 0.0, answers were direct, concise, and factual. At 0.5, they were slightly more conversational. At 1.0, responses were more expressive, occasionally adding metaphors or rhetorical flair, but sometimes at the cost of precision.\n",
        "\n",
        "2. When would you prefer a low temperature versus a high one?\n",
        "Low temperature (e.g., 0.0â€“0.3) is ideal for tasks requiring high accuracy and consistency, such as fact-based answers or summaries. High temperature (e.g., 0.8â€“1.0) is better for creative writing, brainstorming, or generating diverse outputs where precision is less important.\n",
        "\n",
        "3. What happened when the max_output_tokens limit was reached?\n",
        "When the token limit was too low (e.g., 20), the output was clearly cut offâ€”sometimes mid-sentenceâ€”making the answer incomplete. With more generous limits (e.g., 200), the response was more comprehensive and detailed.\n",
        "\n",
        "4. Describe a practical scenario where you would need to set a specific token limit.\n",
        "In applications like chatbots, mobile apps, or SMS services, space is limited. For instance, summarizing a news article to fit within 160 characters would require setting a small token limit. In contrast, generating a report for a PDF export would allow for a much higher token cap.\n",
        "\n",
        "5. How could you combine these two parameters to get a short, creative response versus a long, factual one?\n",
        "- For a **short, creative response**, set a **high temperature (e.g., 0.9)** and a **low max_output_tokens (e.g., 50)**.\n",
        "- For a **long, factual response**, use a **low temperature (e.g., 0.2)** and a **high token limit (e.g., 300 or more)**.\n"
      ],
      "metadata": {
        "id": "Fqn60dhGtK1H"
      }
    }
  ]
}